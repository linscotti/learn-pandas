{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python Data Mastery - A Hands-On Guide to Efficient Data Analysis for Engineers and Students\n",
    "#### Chapter 1 - Python Basics for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "td,th {\n",
    "font-size: 10px    \n",
    "}\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Syntax and Data Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(dataset):\n",
    "    for item in dataset:\n",
    "        if item > 0:\n",
    "            print(\"Posistive value found\")\n",
    "        else:\n",
    "            print(\"Non-positive value found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a NumPy array\n",
    "data = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Creatig pandas series\n",
    "series = pd.Series(data, name=\"Values\")\n",
    "\n",
    "# Creating a pandas DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"ID\": [1, 2, 3, 4, 5],\n",
    "        \"Name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "        \"Score\": [95.5, 87.2, 91.8, 76.9, 88.3],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Control Structures and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Get the CSV file path from the config (relative path)\n",
    "csv_file_path = Path(config[\"sales_data_csv\"])\n",
    "\n",
    "# Check if the file exists\n",
    "if not csv_file_path.exists():\n",
    "    print(f\"File not found: {csv_file_path}\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file_path, sep=\",\")\n",
    "\n",
    "    # Conditional statement to categorize sales\n",
    "    def categorize_sale(amount: int) -> str:\n",
    "        if amount < 100:\n",
    "            return \"Low\"\n",
    "        elif 100 <= amount < 1000:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "\n",
    "    # Apply the categorization to the dataset\n",
    "    df[\"SaleCategory\"] = df[\"SaleAmount\"].apply(categorize_sale)\n",
    "\n",
    "    # Loop through the dataset to calculate total sales by category\n",
    "    category_totals = {\"Low\": 0, \"Medium\": 0, \"High\": 0}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        category_totals[row[\"SaleCategory\"]] += row[\"SaleAmount\"]\n",
    "\n",
    "    print(category_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Changes in the previous code:  \n",
    "\n",
    "Loading the Config File: The path to the CSV file is dynamically loaded from **config_data.json**.\n",
    "\n",
    "No Hardcoding: The CSV file path is no longer hardcoded in the script; it is fetched from the configuration file.  \n",
    "\n",
    "This makes the script more flexible, as you can easily change the file paths by updating the configuration file without modifying the code itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the hardcoded path not using 'pathlib' but 'os'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base_path to the current working directory\n",
    "base_path = os.getcwd()\n",
    "csv_file_path = os.path.join(base_path, \"data/sales_data.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_file_path, sep=\",\")\n",
    "\n",
    "\n",
    "# Conditional statement to categorize sales\n",
    "def categorize_sale(amount):\n",
    "    def categorize_sale(amount: int) -> str:\n",
    "        if amount < 100:\n",
    "            return \"Low\"\n",
    "        elif 100 <= amount < 1000:\n",
    "            return \"Medium\"\n",
    "        else:\n",
    "            return \"High\"\n",
    "\n",
    "\n",
    "# Apply the categorization to the dataset\n",
    "df[\"SaleCategory\"] = df[\"SaleAmount\"].apply(categorize_sale)\n",
    "\n",
    "# Loop through the dataset to calculate total sales by category\n",
    "category_totals = {\"Low\": 0, \"Medium\": 0, \"High\": 0}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    category_totals[row[\"SaleCategory\"]] += row[\"SaleAmount\"]\n",
    "\n",
    "# Print the final result\n",
    "print(category_totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to work with 'pathlib' or 'os' considering the hardcoded path?\n",
    "\n",
    "Both pathlib and os modules in Python are used for handling file paths, but they have different strengths and use cases. Here’s a quick comparison to help you decide when to use each:\n",
    "\n",
    "##### pathlib  \n",
    "\n",
    "Object-Oriented: pathlib provides an object-oriented approach to handling file paths, making the code more readable and intuitive.  \n",
    "\n",
    "Cross-Platform: It automatically handles different path formats across operating systems (e.g., Windows vs. Unix).  \n",
    "\n",
    "Modern Features: Introduced in Python 3.4, it includes many modern features and methods for common file operations.  \n",
    "\n",
    "Path Operations: Simplifies complex path manipulations and makes them more readable.\n",
    "\n",
    "##### os and os.path  \n",
    "\n",
    "Legacy Support: os and os.path have been around for a long time and are well-suited for scripts that need to run on older Python versions.  \n",
    "\n",
    "Procedural Approach: Uses a more traditional, procedural approach to path handling.  \n",
    "\n",
    "Specific Functions: Provides specific functions for various file operations, which can be useful for quick scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the last code using 'match' statement to evaluate the subject which is the value after the match keyword and checks it against patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Get the CSV file path from the config (relative path)\n",
    "csv_file_path = Path(config[\"sales_data_csv\"])\n",
    "\n",
    "# Check if the file exists\n",
    "if not csv_file_path.exists():\n",
    "    print(f\"File not found: {csv_file_path}\")\n",
    "else:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file_path, sep=\",\")\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_file_path, sep=\",\")\n",
    "\n",
    "    # Match statement to categorize sales\n",
    "    def categorize_sale(amount: int) -> str:\n",
    "        match amount:\n",
    "            case amount if amount < 100:\n",
    "                return \"Low\"\n",
    "            case amount if 100 <= amount < 1000:\n",
    "                return \"Medium\"\n",
    "            case _:\n",
    "                return \"High\"\n",
    "\n",
    "    # Apply the categorization to the dataset\n",
    "    df[\"SaleCategory\"] = df[\"SaleAmount\"].apply(categorize_sale)\n",
    "\n",
    "    # Loop through the dataset to calculate total sales by category\n",
    "    category_totals = {\"Low\": 0, \"Medium\": 0, \"High\": 0}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        category_totals[row[\"SaleCategory\"]] += row[\"SaleAmount\"]\n",
    "\n",
    "    print(category_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Get the CSV file path from the config (relative path)\n",
    "csv_file_path = Path(config[\"sales_data_1_csv\"])\n",
    "\n",
    "\n",
    "def load_and_clean_data(csv_file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        df.dropna(inplace=True)\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        print(df.head())  # Check the first few rows\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_monthly_average(df, column_name):\n",
    "    return df.resample(\"ME\", on=\"Date\")[column_name].mean()\n",
    "\n",
    "\n",
    "def plot_monthly_trend(df, column_name, title):\n",
    "    try:\n",
    "        monthly_avg = calculate_monthly_average(df, column_name)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(monthly_avg.index, monthly_avg.values)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Average \" + column_name)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting data: {e}\")\n",
    "\n",
    "\n",
    "# Usage\n",
    "sales_data = load_and_clean_data(csv_file_path)\n",
    "if sales_data is not None:\n",
    "    plot_monthly_trend(sales_data, \"SaleAmount\", \"Monthly Average Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important aspect of Python functions in data analysis\n",
    "is the use of *args and **kwargs. These allow you to create\n",
    "flexible functions that can accept a variable number of\n",
    "arguments. This can be particularly useful when creating\n",
    "analysis functions that need to work with different datasets or\n",
    "parameters:\n",
    "\n",
    "This function can analyze any number of columns and\n",
    "optionally create a plot, demonstrating the flexibility that *args\n",
    "and **kwargs provide.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock DataFrame with random sample data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "data = {\n",
    "    \"SaleAmount\": np.random.randint(\n",
    "        100, 500, size=50\n",
    "    ),  # Random integers between 100 and 500\n",
    "    \"Profit\": np.random.randint(20, 100, size=50),  # Random integers between 20 and 100\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Function to analyze columns\n",
    "def analyze_columns(df, *columns, **kwargs):\n",
    "    for column in columns:\n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "        print(f\"{column}: Mean = {mean:.2f}, Std Dev = {std:.2f}\")\n",
    "\n",
    "    # Check if 'plot' is in kwargs and set to True\n",
    "    if \"plot\" in kwargs and kwargs[\"plot\"]:\n",
    "        df[list(columns)].plot()\n",
    "        plt.title(f\"Plot of {', '.join(columns)}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "analyze_columns(df, \"SaleAmount\", \"Profit\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Handling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the basics of file handling in Python. The fundamental operations involve opening a file, reading from or writing to it, and then closing the file. Python's `with` statement is particularly useful for file operations as it ensures that the file is properly closed after you're done with it, even if an exception occurs. Here's a simple example of reading a text\n",
    "file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Get the CSV file path from the config (relative path)\n",
    "csv_file_path = Path(config[\"data_file\"])\n",
    "\n",
    "with open(csv_file_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis, you'll often work with structured data formats like CSV (Comma-Separated Values). While you can use Python's built-in file handling to read CSV files line by line, the `csv` module provides more convenient tools for working with this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Get the CSV file path from the config (relative path)\n",
    "csv_file_path = Path(config[\"sales_data_csv\"])\n",
    "\n",
    "with open(csv_file_path, \"r\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    headers = next(csv_reader)  # Read the header row\n",
    "    for row in csv_reader:\n",
    "        # Process each row of data\n",
    "        print(f\"Sale ID: {row[0]}, Sale_Amount: {row[1]}\")\n",
    "\n",
    "\"\"\" This example reads a CSV file, skips the header row, and then processes each subsequent row of data.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, for more advanced data analysis tasks, you'll likely use pandas, which provides powerful tools for reading various file formats. Pandas can efficiently read large CSV files and automatically convert them into DataFrame objects, which are ideal for data manipulation and analysis:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "with config_path.open() as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Get the CSV file path from the config (relative path)\n",
    "csv_file_path = Path(config[\"sales_data_csv\"])\n",
    "\n",
    "# Reading a CSV file\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# Basic data exploration\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "# Writing the processed data back to a CSV file\n",
    "df.to_csv(\n",
    "    \"/home/charles/github/repos/learn-pandas/output/processed_sales_data.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to read a CSV file into a pandas DataFrame, perform some basic exploratory data\n",
    "analysis, and then write the processed data back to a new CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an optimized version of your code using pathlib improvements, while maintaining the logic to load paths from a configuration file and simplifying the file handling.\n",
    "\n",
    "In particular, we'll:\n",
    "\n",
    "Replace explicit file opening with Path.read_text() for the config file.  \n",
    "\n",
    "Use Path.write_text() and Path.read_text() consistently.  \n",
    "\n",
    "Make path handling more flexible, including resolving relative paths with config_path.parent to handle all files being in the data folder  \n",
    "\n",
    "Refactor writing the processed CSV to use pathlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%NBQA-CELL-SEP151b14\n",
    "# from my_data_analysis import data_loader, preprocessor, analyzer, visualizer  # type: ignore\n",
    "\n",
    "# You could then use this package in your main script like this:\n",
    "\n",
    "# Load data\n",
    "...\n",
    "config_path = Path(\"data/config_data.json\")  # Path to your config file\n",
    "config = json.loads(config_path.read_text())\n",
    "# Get the CSV file path from the config (relative to the config file's directory)\n",
    "csv_file_path = config_path.parent / config[\"sales_data_csv\"]\n",
    "# Reading the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "# Basic data exploration\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "# Define the output file path for processed data\n",
    "output_file_path = Path(\n",
    "    \"/home/charles/github/repos/learn-pandas/output/processed_sales_data.csv\"\n",
    ")\n",
    "# Writing the processed data back to a CSV file using pathlib\n",
    "df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Improvements:  \n",
    "\n",
    "Using Path.read_text() for Config File:\n",
    "\n",
    "Instead of using with open(), we now use Path.read_text() to load the JSON config directly. This is cleaner and more concise.  \n",
    "\n",
    "Handling Paths Relative to the Config File:\n",
    "\n",
    "The config_path.parent is used to ensure that any relative paths in the config file (like config[\"sales_data_csv\"]) are resolved relative to the location of the config file. This allows the sales_data_csv to be correctly located within the data directory, even if the script is run from another directory.\n",
    "\n",
    "This approach improves robustness when working with paths.\n",
    "\n",
    "Consistent Use of pathlib.Path:\n",
    "\n",
    "The output CSV file path is now defined using Path, making the path management more consistent and extensible.  \n",
    "\n",
    "Code Simplification:\n",
    "\n",
    "By using pathlib consistently, the code becomes more readable and maintainable, as Path objects are more intuitive and Pythonic.  \n",
    "\n",
    "Explanation:  \n",
    "\n",
    "config_path.parent: This gets the parent directory of the config_data.json file, which is the data folder. Using this, we concatenate the relative path (config[\"sales_data_csv\"]) to correctly find the CSV file.\n",
    "\n",
    "Handling Paths in Config Files: If your config specifies relative paths, you want to make sure they are resolved relative to the config file location, not the current working directory. config_path.parent handles that for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON (JavaScript Object Notation) is another common format for storing and exchanging data. Python's `json` module\n",
    "provides tools for working with JSON data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct path to the CSV file\n",
    "json_csv_file_path = Path(\"/home/charles/github/repos/learn-pandas/data/config.json\")\n",
    "\n",
    "with open(json_csv_file_path, \"r\") as file:\n",
    "    config = json.load(file)\n",
    "\n",
    "print(config[\"database\"])\n",
    "# Writing JSON data\n",
    "data = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\n",
    "with open(\"/home/charles/github/repos/learn-pandas/output/output.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a full optimization of your code using Python 3.13 features, specifically focusing on leveraging pathlib for both reading and writing files, and simplifying the file operations for better clarity and efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the JSON configuration file and the output file\n",
    "json_csv_file_path = Path(\"/home/charles/github/repos/learn-pandas/data/config.json\")\n",
    "output_json_file_path = Path(\n",
    "    \"/home/charles/github/repos/learn-pandas/output/output.json\"\n",
    ")\n",
    "# Reading JSON data directly using pathlib's read_text()\n",
    "config = json.loads(json_csv_file_path.read_text())\n",
    "# Print the \"database\" key from the config\n",
    "print(config[\"database\"])\n",
    "# Writing JSON data directly using pathlib's write_text()\n",
    "data = {\"name\": \"Alice\", \"age\": 30, \"city\": \"New York\"}\n",
    "output_json_file_path.write_text(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Optimizations and Enhancements:  \n",
    "\n",
    "Path.read_text() for reading:\n",
    "\n",
    "Instead of using with open(), we use Path.read_text() to read the contents of the file directly into a string, which is then parsed with json.loads().\n",
    "This removes the explicit file-handling code and simplifies the logic.\n",
    "Path.write_text() for writing:\n",
    "\n",
    "Similar to reading, Path.write_text() is used to write the serialized JSON data directly into the output file, without the need for with open().\n",
    "\n",
    "json.dumps() is used to serialize the data before writing, allowing you to format the JSON output with the indent=4 option.\n",
    "\n",
    "Improved readability:\n",
    "\n",
    "By using pathlib methods like read_text() and write_text(), the code becomes more concise and Pythonic, focusing on functionality rather than file handling boilerplate.\n",
    "\n",
    "Advantages of this Optimization:\n",
    "\n",
    "Cleaner code: Removes the need for manual with open() file handling.\n",
    "Leverages new features: Uses Python 3.13's powerful pathlib methods for more elegant file operations.\n",
    "\n",
    "Improved maintainability: The code is easier to maintain and read because it uses high-level abstractions for file I/O."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example shows how to read a JSON configuration file and how to write data to a JSON file. The `indent` parameter in `json.dump()` is used to format the output file for better readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger JSON datasets, pandas again comes to the rescue with its `read_json()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct path to the CSV file\n",
    "json_csv_file_path = Path(\n",
    "    \"/home/charles/github/repos/learn-pandas/data/large_dataset.json\"\n",
    ")\n",
    "\n",
    "# Reading a JSON file into a DataFrame\n",
    "df = pd.read_json(json_csv_file_path)\n",
    "\n",
    "# Processing the data\n",
    "processed_df = df[df[\"stock\"] > 25].groupby(\"city\").mean()\n",
    "\n",
    "# Writing the processed data to a new JSON file\n",
    "processed_df.to_json(\n",
    "    \"/home/charles/github/repos/learn-pandas/output/processed_data.json\",\n",
    "    orient=\"records\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error ValueError: All arrays must be of the same length occurs because pandas.read_json expects a JSON file to be structured in a way that forms a valid DataFrame. However, the structure of your large_dataset.json might not be directly suitable for this. JSON files can have nested or irregular structures, and Pandas requires consistent key-value pairs to form a proper DataFrame.\n",
    "\n",
    "To resolve this issue, you may need to flatten the JSON structure or focus on specific parts of the data. Here's a step-by-step process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check the Structure of the JSON File  \n",
    "\n",
    "First, let's load and inspect the structure of the JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "json_csv_file_path = Path(\n",
    "    \"/home/charles/github/repos/learn-pandas/data/large_dataset.json\"\n",
    ")\n",
    "with open(json_csv_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check the structure of the data\n",
    "print(json.dumps(data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help you understand if the JSON structure is nested or if certain fields need to be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Flatten the JSON Structure  \n",
    "\n",
    "If the JSON has nested structures (like users, products, etc.), you will need to normalize it to a flat structure. You can use pd.json_normalize() to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "json_csv_file_path = Path(\n",
    "    \"/home/charles/github/repos/learn-pandas/data/large_dataset.json\"\n",
    ")\n",
    "with open(json_csv_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize the JSON data\n",
    "df = pd.json_normalize(data[\"products\"])  # or data['users'], depending on what you want\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Modify Your Processing Logic  \n",
    "\n",
    "Once you have the products or relevant part of the dataset flattened into a DataFrame, you can filter and process it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming we are working with 'products' and filtering by 'stock'\n",
    "processed_df = df[df[\"stock\"] > 25].groupby(\"category\").mean()\n",
    "\n",
    "# Writing the processed data to a new JSON file\n",
    "processed_df.to_json(\n",
    "    \"/home/charles/github/repos/learn-pandas/output/processed_data.json\",\n",
    "    orient=\"records\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error TypeError: agg function failed [how->mean,dtype->object] occurs because groupby().mean() expects numerical columns, but your DataFrame contains non-numeric data (e.g., strings or objects like category or other fields).\n",
    "\n",
    "To resolve this, you need to:\n",
    "\n",
    "Ensure that only numeric columns are used in aggregation functions like mean().\n",
    "Exclude non-numeric columns from the mean() calculation or explicitly choose the numeric columns you want to aggregate.  \n",
    "\n",
    "Here’s a revised approach:\n",
    "\n",
    "1. Exclude Non-Numeric Columns for Aggregation\n",
    "You can filter out the non-numeric columns before applying mean():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'category' and select only numeric columns for aggregation\n",
    "numeric_columns = df.select_dtypes(include=\"number\").columns\n",
    "processed_df = df[df[\"stock\"] > 25].groupby(\"category\")[numeric_columns].mean()\n",
    "\n",
    "# Writing the processed data to a new JSON file\n",
    "processed_df.to_json(\n",
    "    \"/home/charles/github/repos/learn-pandas/output/processed_data.json\",\n",
    "    orient=\"records\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check the Data Types  \n",
    "\n",
    "It’s useful to check the data types in your DataFrame to see which columns are causing issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will help you identify non-numeric columns, so you can exclude them from the aggregation step.\n",
    "\n",
    "Example Code with Full Flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "json_csv_file_path = Path(\n",
    "    \"/home/charles/github/repos/learn-pandas/data/large_dataset.json\"\n",
    ")\n",
    "with open(json_csv_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Normalize the 'products' part of the data\n",
    "df = pd.json_normalize(data[\"products\"])\n",
    "\n",
    "# Check the data types\n",
    "print(df.dtypes)\n",
    "\n",
    "# Filter by stock and group by 'category', aggregating only numeric columns\n",
    "numeric_columns = df.select_dtypes(include=\"number\").columns\n",
    "processed_df = df[df[\"stock\"] > 25].groupby(\"category\")[numeric_columns].mean()\n",
    "\n",
    "# Write the processed data to a new JSON file\n",
    "processed_df.to_json(\n",
    "    \"/home/charles/github/repos/learn-pandas/data/processed_data.json\", orient=\"records\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:  \n",
    "\n",
    "select_dtypes(include='number'): This filters out only the numeric columns (like stock, price, etc.).\n",
    "\n",
    "groupby('category')[numeric_columns].mean(): Groups the data by category and calculates the mean of only numeric columns (e.g., stock, price).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with very large datasets that don't fit into memory, you may need to process files in chunks. Pandas provides options for this as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000  # Number of rows per chunk\n",
    "chunks = []\n",
    "# Read and process the file in chunks\n",
    "for chunk in pd.read_csv(\"very_large_file.csv\", chunksize=chunk_size):\n",
    "    # Process each chunk\n",
    "    processed_chunk = chunk[chunk[\"value\"] > 0].copy()\n",
    "    processed_chunk[\"squared\"] = processed_chunk[\"value\"] ** 2\n",
    "    chunks.append(processed_chunk)\n",
    "# Combine all processed chunks\n",
    "result = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach allows you to process large files that exceed your system's memory capacity, which is a common scenario in big data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to CSV and JSON, Python and pandas support many other file formats commonly used in data analysis,including Excel files (`.xlsx`), SQL databases, and more specialized formats like HDF5 for large scientific datasets.\n",
    "\n",
    "Here's a quick example of reading an Excel file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading an Excel file\n",
    "df = pd.read_excel(\"financial_data.xlsx\", sheet_name=\"Q3_Sales\")\n",
    "# Writing to an Excel file\n",
    "df.to_excel(\n",
    "    \"processed_financial_data.xlsx\", sheet_name=\"Processed_Q3_Sales\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a module in your Python script, you use the `import` statement. Here's an example using the built-in `math` module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 5\n",
    "area = math.pi * math.pow(radius, 2)\n",
    "print(f\"The area of a circle with radius {radius} is {area:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we import the entire `math` module and use its `pi` constant and `pow()` function. You can also import specific functions or constants from a module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 5\n",
    "area = pi * radius**2\n",
    "circumference = 2 * pi * radius\n",
    "print(f\"Area: {area:.2f}, Circumference: {circumference:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame({\"x\": np.linspace(0, 10, 100), \"y\": np.sin(np.linspace(0, 10, 100))})\n",
    "# Perform some calculations\n",
    "df[\"y_squared\"] = np.square(df[\"y\"])\n",
    "# Create a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[\"x\"], df[\"y\"], label=\"sin(x)\")\n",
    "plt.plot(df[\"x\"], df[\"y_squared\"], label=\"sin^2(x)\")\n",
    "plt.legend()\n",
    "plt.title(\"Sine and Squared Sine Functions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script demonstrates how different packages can work together in a data analysis workflow: NumPy for numerical operations, pandas for data manipulation, and matplotlib for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As your data analysis projects grow in complexity, you may find it useful to create your own modules and packages to organize your code. Here's an example of how you might structure a simple data analysis package:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_data_analysis/  \n",
    "`__init__.py`  \n",
    "data_loader.py  \n",
    "preprocessor.py  \n",
    "analyzer.py  \n",
    "visualizer.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this structure:  \n",
    "\n",
    "`__init__.py` is an empty file that tells Python this directory should be treated as a package.\n",
    "\n",
    "`data_loader.py` might contain functions for loading data from various sources.\n",
    "\n",
    "`preprocessor.py` could include functions for cleaning and preparing data.  \n",
    "\n",
    "`analyzer.py` might have functions for statistical analysis and modeling.  \n",
    "\n",
    "`visualizer.py` could contain functions for creating various types of plots and charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You could then use this package in your main script like this:\n",
    "from my_data_analysis import data_loader, preprocessor, analyzer, visualizer  # type: ignore\n",
    "\n",
    "# Load data\n",
    "data = data_loader.load_csv(\"sales_data.csv\")\n",
    "# Preprocess\n",
    "clean_data = preprocessor.clean(data)\n",
    "normalized_data = preprocessor.normalize(clean_data)\n",
    "# Analyze\n",
    "results = analyzer.perform_regression(normalized_data)\n",
    "# Visualize\n",
    "visualizer.plot_regression_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modular approach makes your code more organized,reusable, and easier to maintain.\n",
    "When working with modules and packages, it's important to be aware of Python's import system and how it resolves module names. Python looks for modules in the following order:\n",
    "\n",
    "The directory containing the script being run  \n",
    "\n",
    "The Python standard library  \n",
    "\n",
    "The list of directories specified in the PYTHONPATH environment variable  \n",
    "\n",
    "The site-packages directory where third-party packages are installed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful feature when working with modules is the ability to run a module as a script. This is commonly done by including a block like this at the end of your module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Code to run when this module is executed directly\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows you to include code that runs only when the module is executed directly, not when it's imported by another script. This can be useful for including examples or test code in your modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you develop your skills in data analysis with Python, you'll likely encounter and use many different modules and packages. Some key packages to be familiar with include:\n",
    "\n",
    "NumPy: For numerical computing and working with arrays\n",
    "\n",
    "pandas: For data manipulation and analysis\n",
    "\n",
    "matplotlib and seaborn: For data visualization \n",
    "\n",
    "scikit-learn: For machine learning and statistical modeling\n",
    "\n",
    "SciPy: For scientific computing\n",
    "\n",
    "statsmodels: For statistical computations and econometric models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas-kernel",
   "language": "python",
   "name": "pandas-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
